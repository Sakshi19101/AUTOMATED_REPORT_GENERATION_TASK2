# AUTOMATED_REPORT_GENERATION_TASK2
COMPANY : CODTECH IT SOLUTONS
NAME : SAKSHI SALUNKHE
INTERN ID : CT04DF2845
DOMIAN : PYTHON PROGRAMMING
DURATION : 4 WEEKS
MENTOR : NEELA SANTOSH KUMAR
DESCRIPTION OF TASK2 : Internship Task 2 involved the automated generation of a summary statistical report from structured CSV data, particularly student scores across various subjects such as Math, Science, and English. The task began with reading data from a CSV file named sample_data.csv, which contained numeric values representing scores of different students. The objective was to process this data, compute meaningful statistical summaries including count, mean, standard deviation, minimum, maximum, and key percentiles like 25%, 50%, and 75%, and then present these results in a structured, readable PDF format. This process was implemented in Python using two essential libraries: pandas for data processing and analysis, and fpdf for PDF generation. The script first imported the required modules and loaded the CSV data using pandas.read_csv. It then generated a summary of the data using the .describe() method, which computes descriptive statistics for numeric columns. To create the PDF, a custom class was defined by extending the FPDF class, allowing customization of headers, titles, and body content. The report included a centralized title "Automated Report" and a section titled "Summary Statistics" followed by the tabular output of the .describe() method. This PDF report was saved as report.pdf using pdf.output("report.pdf"). The script was executed with a simple python task2.py command from the terminal, assuming the CSV and script were located in the same directory. The PDF output could then be used for review, submission, or documentation purposes. The task demonstrated practical use of Python in automating data reporting workflows and provided hands-on experience with real-world data handling and document creation. The libraries used were installed using the commands pip install pandas and pip install fpdf. This task reinforced the concepts of data ingestion, transformation, analysis, and presentation in a professional format. The approach ensures scalability for larger datasets and adaptability for different data schemas by modifying the CSV structure and adjusting the script accordingly. Repeating this process helped in solidifying the workflow from raw data to polished output. Furthermore, the task encouraged clean code practices by using function definitions and object-oriented principles via the class-based PDF generation. The chapter_title() and chapter_body() methods allowed for structured formatting within the PDF, ensuring that the report was both machine-generated and human-readable. It was an exercise in automation, accuracy, and report standardization. The sample dataset included entries such as Alice with scores 85 in Math, 88 in Science, and 92 in English; Bob with 78, 74, and 81 respectively; Charlie with 90, 92, and 93; David with 72, 70, and 75; and Eve with 88, 90, and 87. The resulting summary showed a mean score of approximately 82.6 in Math, 82.8 in Science, and 85.6 in English. This single-task encapsulated the essence of turning raw data into insights and automated documents. Repeating these steps across domains such as finance, HR, healthcare, or education could lead to scalable automated reporting systems. The combination of simplicity in logic and power in output makes this task particularly impactful for anyone looking to build automated systems. Task 2 also laid the foundation for learning PDF formatting intricacies such as line breaks, alignment, font selection, and spacing. In essence, this task was not just a coding challenge but a lesson in building reproducible, automated workflows for data analysis and professional reporting. From setting up the environment, writing efficient Python code, to executing and producing polished output, Task 2 covered the entire lifecycle of a basic data project. The final PDF contained clear, structured content suitable for presentations or formal submissions. Repeating this structure across multiple datasets further helped in reinforcing consistency. The hands-on experience with real data and live script execution elevated understanding of both Python and data science practices. Overall, this internship task was highly educational, practical, and aligned with real-world applications in data-driven fields. The ability to convert CSV data to a clean PDF report through a few lines of Python code marks a significant step toward building efficient data pipelines. This task was not only about coding but also about conceptual clarity, structured programming, and applying tools to solve meaningful problems. It showcased how open-source libraries like pandas and fpdf can be leveraged to build automated solutions without the need for complex software stacks. The methodology is replicable, the skills are transferable, and the results are directly useful for teams and stakeholders needing instant statistical summaries. This single script bridges the gap between raw tabular input and refined, consumable output in PDF format, reinforcing the importance of automation in modern workflows. This task also promoted best practices in script modularity and encouraged exploration of further enhancements such as adding charts or exporting multiple reports dynamically. With this foundation, one could extend the script to accept command-line arguments for input/output files, add error handling for file availability, or integrate visual plots using libraries like matplotlib. The learning curve, though beginner-friendly, introduces a significant array of techniques relevant to data science, software engineering, and business analysis. Thus, Internship Task 2 successfully encapsulated the intersection of data processing, automation, and documentation â€” providing a robust, hands-on platform to develop essential professional skills in programming and data reporting.
OUTPUT : [report.pdf](https://github.com/user-attachments/files/20684160/report.pdf)
